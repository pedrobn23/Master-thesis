
\chapter{Martin-L\"of theories and Seely's equality}
\label{chap:5}
\thispagestyle{empty}

 In this we use hyperdoctrines as a tool to better understand closed Cartesian categories and relate them with a new typing theory: Martin-Lof type theory.\\

 In this chapter we continue our categorical presentation of the tip theory with the introduction of Martin Lof's type theory. This type theory, also known as intuitionistic type theory, is initially proposed as a foundation of mathematics. This is really important because it implies a change of mentality, as we do no longer are so focus formalizing computation systems. \\

This relationship with first-order logic arises from the hand of dependent typing. That is, in Martin Lof's type theory there is a formal structure for generating types dependent on other types. This is used (without needing to know the formalism) nowadays continuously in programming languages. \\

The most interesting part for us will be to study the new structures that arise in the theory of categories to understand this new typing. In particular, we will study local closed Cartesian category structures through hyperdoctrines. \\

The main sources for this chapter are  \cite{seely1984locally}, \cite{martinlof1973intuitionistic}  and \cite{mac2013categories}



\section{Martin-Löf type theory}

In this section we introduce rather succinctly the foundations of the type theory. This presentation follows the one done in \cite{seely1984locally}, with added insights and remarks form \cite{martinlof1973intuitionistic} and \cite{sep-type-theory-intuitionistic}.



\subsection{First definition of Martin-Löf type theory}

After having previously introduced the calculations, in this one we will introduce Martin-L\"of type theory, ML for short, in a more systematic way, introducing type formation rules, term formation rules, and the equivalence rules. After that, we will introduce the category structure. The main source for this section is \cite{martinlof1973intuitionistic}, with summaries taken from \cite{seely1984locally} with commentary from .\\

As a founding level of mathematics, we are going to define types and terms as before. The change in enphasis comes from the fact that we will manage the notation of the Curry-Howard from the beginning, as it was done while firstly introduced by Martin-L\"of in \cite{martinlof1973intuitionistic}. In this we will always consider \emph{types as propositions} and \emph{terms as proofs}, and we will made use of this analogy to explain the properties of the theory. Also, as done in lambda-calculus, we should regard also \emph{proofs as objects}, with all the constructions previously done still in place.\\

One last particularity of the Martin-L\"of is the fact that we can include details of proofs in the statement of propositions. This will be formalized further on, but explain why we need to delay the introduction of term and type rules until the definition of type-function.

\subsubsection{Terms and types}

The formal system that Martin-L\"of presents consists of a set of rules $a : A$, as could be predicted, that a \emph{term $a$ is of type symbol $A$}, that means that $A$ as a proposition is proved by the proof $a$. We also consider equality between term $a = b$ and equality between types $A = B$. Types will be preferably denoted by $A,B,C...$ and terms by $a,b,c$ characters..\\

After defining the abstract concept of term and types, and before introducing the full term and type forming rule, we will introduce the concept of \emph{variables} and \emph{function constant}:

Variables are defined analogously as in the case of simply typed lambda calculus. The statements $x: A$ represents arbitrary object of a given type. As previously, an statement of $x: A$ where $x$ is a variable is called an \emph{assumption}. We will also note the type of variables as super-index, $x^A$. As in simply typed lambda calculus, a term $a$ can depend on variables $x_1,...,x_n$, usually noted by $a(x_1,...,x_n)$.\\

  Quite importantly, as types can depend on variables, and each variable should have a type, we need to impose the \emph{condition on variables}. This state that in statement $a(x_1,..,x_n)$ it should happens that no variable $x_j$ should depend on variables $x_i$ for any $i<j$. The results of substituting $x_1:A_1,...,x_n:A_n$ with $a_1:A_1,...,a_n:A_n$ is denoted by $a(a_1,...,a_n)$. Note that in this last expression, $A_n$ can depend on $x_1,...,x_{n-1}$, thus noted $A[x_1,...,x_{n-1}]$. Variables will be prefereably denoted by $x,y,z$ characters. Further on, each time we talk about any generic type or term dependent on variables, we will always suppose the it follows the condition on variables.\\

Note that type can depend on variables, that is, on terms. They do not depend on other types directly.\\ 
  
Function constant is a function from terms to terms. Each function constant $f$ has associated an index $n$ with the number of argument, and also $n$ types symbols $$A_1, A_2[x_1^{A_1}],...,A_n[x_1^{A_1},...,x_1^{A_{n-1}}].$$ A function $f$ of index $n$ is called to be an \emph{$n$-ary function constant}. A 0-ary function is called a \emph{constant}.  Function constants are preferably denoted by $f,g,h$ characters. \\

In addition to function constant, we can consider \emph{type valued function constants}. This have the same indicacion as above, and a 0-ary type valued function constant is called a type constant. In the analogy of terms and types, with proof and proposition, that have previously been discussed, type valued functions constants are \emph{properties}. Type valued function constants are preferably denoted by $F,G,H$ characters. By opposition, function constants are called \emph{terms valued function constants}.


\begin{example}\label{example:primeML}
Let $\N$ be the type of every natural number and the type valued function $P(x)$ be the function that represents the property "being a prime number". We can express the fact that 3 is a natural number as $3: \N$. We can then consider  type $P(3)$ will be the proposition ``3 is a prime number''. 
\end{example}

\subsubsection{Full typing structure}
Now we are in place to describe the formation rules.

\begin{definition}[Type formation rules, 1.1.1 \cite{seely1984locally}]
  The following are to be terms of the indicate types:
  \begin{enumerate}
  \item 1 is a type.
  \item If $F(x_1,..,x_n)$ is a type-valued function constant, then $F(a_1,...,a_n)$ is a type, with  $x_1:A_1,...,x_n:A_n$ with $a_1:A_1,...,a_n:A_n$.
  \item If $a,b$ are terms, then $I(a,b)$ is a type.
  \item If $A$ is a type and $B(x^A)$ are types, then $\prod x^A. B(x^A)$ and  $\sum x^A. B(x^A)$
  \end{enumerate}
\end{definition}

\begin{definition}[Term formation rules, 1.1.2 \cite{seely1984locally}]\label{1.1.2-seely}
  The following are to be types:
  \begin{enumerate}
  \item $*:1$.
  \item For every type $A$, there exists countable many variables $x_i^A : A$, $i\in \N$.
  \item If $f(x_1,..,x_n)$ is a term-valued function constant, then $f(a_1,...,a_n)$ is a term of appropriate type, with  $x_1:A_1,...,x_n:A_n$ with $a_1:A_1,...,a_n:A_n$.
  \item If $t(x^A) : B(x^A)$, then:
    $$\lambda x^A.t(x^A) : \prod x^A. B(x^A).$$

    Conversely, if $f : \prod x^A. B(x^A)$ and $a:A$, $f(a) : B(a)$ and $(\lambda x^A.t(x^A))(a) : B(a)$.
  \item If $a:A$ and $b:B(a)$, then $\langle a, b\rangle : \sum x^a. B(x)$. Conversely, $c : \sum x^a. B(x)$, $\pi (c) : A$ and $\pi'(c) : B(\pi(c))$. 
  \item If $a:A$, then $r(a): I(a,a)$. If $a,b : A$, $c : I(a,b)$, $d : C(a,a,r(a))$, where $C(x^A,y^A,z^{I(a,a)})$, then $\sigma_d[a,b,c] : C(a,b,c)$.  
  \end{enumerate}
\end{definition}


We can now explain the intuition behind this structures. We can start with the \emph{Cartesian product of a family of types}. This is represented by $\prod x^A. B(x^A)$. In the analogy proposition-types, we have that this represents the statement $\forall x \in A : B(x)$. Similarly, a proof of such a statement is a function that take an arbitrary object $a:A$ and return a proof for $B(a)$, and thus we define the terms of this types as lambdas $\lambda x^A.t(x^A) : \prod x^A. B(x^A).$.\\


We follow explaining the \emph{disjoint union of a family type}, that is denoted by $\sum x^A. B(x^A)$ and represents the statement $\exists x \in A : B(x)$. As this type of statement is solved by providing an object and a proof of such statement, we thus define the terms of this typed as pairs $\langle x^A, y^{B(x^A)}\rangle$ of an element and its associated proof. As every pair type, it has projections $\pi,\pi'$. This type is also understood as the type of every element of $A$ such that $B(x)$ holds.

\begin{example}
 The set of every real number is:

    $$\R := \left(\sum x^{\N\to\QQ}\right).\left(\prod n^\N\right).\left(\prod m^\N\right).\left(|x_{m+n} - x_m| \le 2^{-m}\right),$$
    
  where in the second example we use the density property of $\QQ$ in $\R$.\\
\end{example}

In the particular case of $B(x)$ having the same type $B$ for every $a:A$, we denote:
$$A\to B:= \prod x^A. B(x^A),\qquad A\times B := \sum x^A. B(x^A).$$

Finally, as a mayor difference from previously, we can consider the identity type $I(a,b)$, that represent the proposition of $a$ and $b$, now regarded as object, are identical, with $r(a)$ being the proof of reflexivity of $a$. 


\begin{remark}
  As it happens in the previous chapter, we can consider an expansion with the sum type of $A+B$, as is presented in \cite[Section 1.6]{martinlof1973intuitionistic}. In the shake of brevity, we avoid it as it is not interesting from a categoric viewpoint.
\end{remark}

\subsection{Equivalences and universes}
\subsubsection{Equivalences}

The most important difference in this exposition of type theory is that now we will not present the differentiated alpha/beta/eta equivalences, but we will present them all together as a single equivalence concept. This is due to two reasons. The first is that we consider that the objective of explaining the intuition and the multiple forms of equality is already satisfied. The second is that our main interest is in the relation to Category Theorics, where we always consider all equivalences together.


\begin{definition}[Equality rules, 1.1.3 \cite{seely1984locally}]
  Using the notation from \ref{1.1.2-seely} point by point:
  \begin{enumerate}
  \item If $t:1$, then $t=*$.
  \item There is no equality rule imposed on variables.
  \item Every imposed equation in functions constants, translate to every interpretation of the function constant.
  \item $(\lambda x^A.t(x^A))(a) =  t(a)$, Also, we can consider function constants as terms, with the obvious equality $f = lambda x^a.f(x)$.
  \item $\pi(\langle a,b\rangle) = a$,  $\pi'(\langle a, b\rangle) = b$, $c=\langle \pi(c),\pi'(c)\rangle$.
  \item $\sigma_d[a,a,r(a)]=d$, if $f(a,b,c)\in C(a,b,c)$ then $f=\sigma(f(a,a,r(a)))(a,b,c)$. Lastly, if $a(x), b(x):A(x)$ and $t(x) : I(a(x),b(x))$, then $a(x)=b(x)$ and $t(x)=r(a(x))$.
  \end{enumerate}

  Finally, we have the  usual internal coherence rules. That is, is reflexive, transitive and symmetric In addition, if $a=b : A$, and $c:B(x^A)$ then $c(a)=c(b)$ and $a:A, b:B, a=b$ implies $A=B$.
\end{definition}

%\footnote{Reflexive, transitive and symmetric properties are not needed in the defintion and in fact can be derived from 6\cite[1.3]{seely1984locally}. We nonetheless included them as a natural part of the definition of equality to favor readability.}.
\subsubsection{Seely's enlargement}

After having presented the Martin-L\"of as presented originally, in this subsection we will explain the modifications that Seely does to the structure. This modifications appear in Seely's previous works \cite{seely1977hyperdoctrines}\footnote{Unlike the other works cited, Seely's doctoral thesis was not available digitally. We have only found available a physical version from the Cambridge Library and thus we were not able to read it. We trust its contents to the references made by Seely himself in his other works cited.} and \cite{seely1983hyperdoctrines}. Seely introduce the following definition:

\begin{definition}
  Let $z : \sum x^A.B(x)$ be a variable, and $C(z)$ a type that on $z$ but not on $x:A$ neither on $y:B(x^A)$. Then, for every $t(x,y) : C(\langle x, y\rangle)$ there exists $\tilde t (z) : C(z)$, defined by:
  $$\tilde t (z) := t(\pi(z),\pi'(z)): C(z).$$
\end{definition}

The point of this notion is simple, that is to more succinctly denote whenever a variable does not depend on a particular pair of variable but only on the moment when they work together. One particular example of this are the properties assign to satisfiable formulas.

With this definition, mantaining the notation, we also introduce the following equalities:
\begin{itemize}
\item $\tilde t(\langle x,y\rangle) = t(\langle x,y\rangle) $.
\item If $f(z): C(z)$, $t(x,y):C(\langle x,y\rangle)$ and $f(\langle x,y\rangle) = t(x,y)$ then $f=\tilde t$.

\end{itemize}


\subsubsection{Universes}
Now we will have a word on \emph{universes}. TODO Sunday.

\subsection{Category of Martin-Löf type theory}



\section{Local Cartesian closed categories}

this new typing system will be related to the categorical structure "local closed Cartesian categories". Instead of following the classical process of introducing the concept formally so that in the course of a technical demonstration one can see why they are related, we believe that there is extra value in explaining the idea behind the formalism.\\


What is the key reason why ML can be considered as a larger structure? Indeed, it is the part by which we can include information about the terms in the typed ones, giving real information about the needs to make a proof system that underlies the mathematics. With this in mind we have to ask ourselves how we can modify the closed Cartesian Category structure so that it pairs with this new typing.  The key idea then comes in considering slice categories (see Definition \ref{def:slice-cat})\footnote{Historically, the actual process was relating first order with LCC, and after that doing the same with ML theories.}.\\

This allow us to consider arrows (terms) as objects (types) in local contexts, being (a bit roughly) translatable to consider objects dependent on arrows.  As many things in category theory, once you have the idea you still have to lay down the structures carefully so that everything relates properly. Lets start with the process.

\begin{definition}
  A category is said to have \emph{finite limits} if it has every limit over any finite category, that is, over any category $J$ with both $Ob(J)$ and $Ar(J)$ finite.
\end{definition}

\begin{definition}
A \emph{locally Cartesian closed category}, LCC for short,  is a category $C$ with finite limits, such that for any object $a$ of $C$, the slice category $C/a $ is Cartesian closed.
\end{definition}

\begin{remark}
  If a category $C$ has finite limits, thus every category $C/a$ with $a\in Ob(C)$  has also finite limits. This needs not to happen with exponent object, being this the main quality of LCCs.
\end{remark}


\subsection{Hyperdoctrines}
\subsubsection{Definition of hyperdoctrine}
As a tool for dealing with LCC we are going to introduce hyperdoctrines. This were introduced in by Lawvere as part of his influential work \cite{lawvere1969adjointness}. The notion of a hyperdoctrine is essentially an axiomatization of the collection of slices of a locally cartesian closed category.

\begin{definition}
  Let $C$ be a category with finite limits. A $C$-indexed category $P$ is a collection that  consists of:
  \begin{enumerate}
  \item For each $a\in Ob(C)$  a category $P(a)$.
  \item For each $f:a\to b, g:b\to c\in Ar(C)$, a functor $P(f) = f^*:P(B) \to P(A)$, such that $(1_a)^* = 1_{P(a)}$ for every  $a \in Ob(C)$ and $(gf)^*= f^*g^*$.
  \end{enumerate}
\end{definition}
\begin{remark}
  Any category $C$ with finite limits, is self-indexed, with $C(a)=C/a$ and $C(f)$ defined by pullback, for every $a\in Ob(C), f\in Ar(C)$. 
\end{remark}
  We can see that $P$ is, in summary, an contravariant functor from $C$ to $Cat$. Properties from $(\cdot)^*$ derive from the fact that arrows in $Cat$ are functors. Therefore a $C$-indexes category is just an object of $Cat^C$.\\

Notation $f^*$ is chosen by Seely as a reminiscent of the pullback notation on Topos theory.


\begin{definition}
  A $C$-indexes category $P$ is a hyperdoctrine if:
  \begin{enumerate}
  \item $P(a)$ is closed Cartesian for every $a\in Ob(C)$.
  \item $f^*$ preserves exponent for every $f\in Ar(C)$.
  \item $f^*$ has adjoints $\sum_f \dashv f^* \dashv \prod_f$.
  \item $P$ satisfies the Beck condition: if the following diagram is a pullback in $C$, then for any object $\varphi$ of $P(c)$, $\sum_k h^*(\varphi) \cong f^*\sum_g(\varphi)$.
    \[
      \begin{tikzpicture}
        \node {\begin{tikzcd}
            d\ar[r,"h"]\ar[d,"k"] & c\ar[d,"g"]\\
            a\ar[r,"f"] & b.
          \end{tikzcd}};
      \end{tikzpicture}
    \]
  \end{enumerate}
\end{definition}
\begin{definition}
  Two $C$-indexed categories $P_1$ and $P_2$ are equivalents if:
  \begin{itemize}
  \item For each $a\in Ob(C)$, $P_1(a)\cong P_2(a)$.
  \item For each $f\in Ar(C)$, $P_1(f)(a)\cong P_2(f)(a)$.
    DUDA: Mirar 2.6 en seely y comentarselo a Angel.
  \end{itemize}
\end{definition}

Note that if $P_1\cong P_2$ as $C$-indexed categories, and $P_1$ is a hypedoctrine, the $P_2$ is an hyperdoctrine.


\subsubsection{Caracterization of LCC}
\begin{theorem}
  If $C$ has finite limits, then $C$ is a LCC if, and only if, as a $C$-indexed category $C$ is a hyperdoctrine. 
\end{theorem}

This result, as noted in \cites{seely1984locally}, can be deduced from results showed in Freid's famous work ``Aspect of Topoi'', \cite[Section 1.3]{freyd1972aspects}. Nonetheless, this implication is not trivial. Thus, we now proceed to detail a bit this proof. During this discussion $C$ is a category, $b\in Ob(C)$ and $f:a\to b\in Ar(C)$.

\begin{definition}[\cite[Section 2]{nlab:reflective_subcategory}]
Definition 2.1. A full subcategory $i:C\hookrightarrow D$ is reflective if the inclusion functor $i$ has a left adjoint $T\dashv i$:
  \[
    \begin{tikzcd}
      C\arrow[hookrightarrow, shift left=.5ex, "i"{name=T}]{r}{} &
      D\arrow[ shift left=1ex, "T"{name=i}]{l}{} \\
    \end{tikzcd}
  \]
The left adjoint is sometimes called the reflector,
\end{definition}

\begin{example}

  $Ab$ is a reflective subcategory of $Grp$, with reflector given by the abelianization of a group.
\end{example}


\begin{proposition}[Section 2, \cite{nlab:reflective_subcategory}]
In reflective subcategories the inclusion $i:C\hookrightarrow D$ creates all limits of $D$ and $C$ has all colimits which $D$ admits.
\end{proposition}


\begin{proposition}[Proposition 1.31,\cite{freyd1972aspects}]
Let $A$ be a CCC and $A'\subset A$ be a full reflexive subcategory with reflector  $R$. Then $R$ preserves products iff for all $b,a\in Ob(A')$, we have that $b^a\in A'$.
\end{proposition}
\begin{proposition}
Let $C$ be a category. We define the functor $\sum_b : C/b \to C$ as the forgetful functor for every $b\in Ob(C)$. Then $\sum_b$ preserves and reflects colimits, equalizers, pullbacks and monomorphisms.
\end{proposition}

If $C$ has finite products, we can define the functor $\times_b: C \to C/b$ such that:
\begin{itemize}
\item For each $a\in Ob(C)$, $\times_b(a) := \pi':a\times b \to b$.
\item For each $f\in Ar(C)$, $\times_b(f) := f\times 1_b: a\times b \to f(a)\times b$.
\end{itemize}

We can check that $\sum_b \dashv \times_b$. Finally, we have the following proposition:

\begin{proposition}
  
\end{proposition}


